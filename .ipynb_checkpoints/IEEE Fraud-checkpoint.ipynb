{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.layers import Input, Dense, Activation, Flatten, Dropout, Embedding, concatenate,multiply,add\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l1\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import unitnorm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The below data comes from the Kaggle IEEE fraud detection competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/michaelspillane/Downloads/ieee-fraud-detection/train_transaction.csv')\n",
    "df_id = pd.read_csv('/Users/michaelspillane/Downloads/ieee-fraud-detection/train_identity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I merge the two data frames on the TransactionID using a left joing because not all entries in df and in df_id.  Next I convert the DateTime to several catagorical variables that may be of use when modeling.  The label is stored in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF = pd.merge(df, df_id,how='left', on='TransactionID')\n",
    "\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "DF['TransactionDT'] = DF['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "\n",
    "DF['dow'] = DF['TransactionDT'].dt.dayofweek\n",
    "DF['dow']= DF['dow'].apply(str)\n",
    "DF['hour'] = DF['TransactionDT'].dt.hour\n",
    "DF['hour']= DF['hour'].apply(str)\n",
    "DF['day'] = DF['TransactionDT'].dt.day\n",
    "DF['day']= DF['day'].apply(str)\n",
    "\n",
    "dtyp = DF.dtypes\n",
    "\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I also extract the cents and tenths of cents from transaction amount as that too coud be useful.  Some of the variables distributions are not gaussian and by taking their log the distributions become more Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF['digit1'] = np.floor((DF['TransactionAmt'].round(decimals=3)%1)*10).astype(int)\n",
    "DF['digit2'] = np.floor(((10*DF['TransactionAmt'].round(decimals=3))%1)*10).astype(int)\n",
    "DF['digit3'] = (np.floor(((100*DF['TransactionAmt'].round(decimals=3))%1)*10).astype(int)>.5)\n",
    "DF['logTransactionAmt'] = np.log(DF['TransactionAmt'])\n",
    "DF['logC1'] = np.log(DF['C1']+1)\n",
    "DF['logC2'] = np.log(DF['C2']+1)\n",
    "DF['logC3'] = np.log(DF['C3']+1)\n",
    "DF['logC4'] = np.log(DF['C4']+1)\n",
    "DF['logC5'] = np.log(DF['C5']+1)\n",
    "DF['logC6'] = np.log(DF['C6']+1)\n",
    "DF['logD1']  = np.log(DF['D1']+1)\n",
    "DF['logD10']  = np.log(DF['D10']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I perform a train test split on the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF_train, DF_test, y_train, y_test = train_test_split(DF, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From the Kaggle website we can find a list of which features are catagorical in nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l = list(DF)[4:13]+list(DF)[15:17]+list(DF)[46:55]+list(DF)[405:]  #catagorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I will use an embedding layer from keras for the catagorical variables and so I seperate them into their own dateframe here.  Some of the numerical features only have a couple of values and so I add them as potential catagorical features though I do not remove them from the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DF_train_time = DF_train['TransactionDT']\n",
    "cat_var_train = DF_train[l]\n",
    "DF_train = DF_train.drop(l,axis = 1)\n",
    "DF_train = DF_train.drop(['TransactionID','isFraud','TransactionDT'],axis = 1)\n",
    "\n",
    "nunq = DF_train.nunique(axis=0)\n",
    "\n",
    "l2 = list(nunq[(nunq<15).values].index)\n",
    "\n",
    "emb_train = pd.concat([cat_var_train,DF_train[l2]],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I impute the missing values as the median of the column and store the imputer for use on the testing set.  I then scale the columns using a standard scaler and finally scale the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = DF_train.columns\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "DF_train = imp.fit_transform(DF_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train  = scaler.fit_transform(DF_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelspillane/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype datetime64[ns] was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/michaelspillane/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype datetime64[ns] was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler_t = StandardScaler()\n",
    "X_scaled_time_train  = scaler_t.fit_transform(DF_train_time.values.reshape(-1, 1))\n",
    "\n",
    "emb_train = emb_train.applymap(str)\n",
    "\n",
    "emb_train[pd.isnull(emb_train)]  = 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we use the above preprocessing on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelspillane/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1494: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "DF_test_time = DF_test['TransactionDT']\n",
    "\n",
    "cat_var_test = DF_test[l]\n",
    "DF_test = DF_test.drop(l,axis = 1)\n",
    "DF_test = DF_test.drop(['TransactionID','isFraud','TransactionDT'],axis = 1)\n",
    "\n",
    "emb_test = cat_var_test.loc[:,list(emb_train)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Checking to make sure that all the columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col2 = DF_test.columns\n",
    "list(set(col)-set(col2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Performing the same imput and scaling used on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelspillane/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype datetime64[ns] was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "DF_test = imp.transform(DF_test)\n",
    "\n",
    "DF_scaled_test  = scaler.transform(DF_test)\n",
    "\n",
    "X_scaled_time_test  = scaler_t.transform(DF_test_time.values.reshape((-1,1)))\n",
    "\n",
    "emb_test = emb_test.applymap(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I encode the catagorical variables into integers for use by keras enbedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "\n",
    "fit = emb_train.apply(lambda x: d[x.name].fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And then use the same mapping for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fit_max = fit.max(axis=0)\n",
    "fit_test = copy.deepcopy(emb_test)\n",
    "\n",
    "for i in list(emb_test):\n",
    "    fit_test[i] = emb_test[i].apply(lambda x: x if x in set(d[i].classes_) else 'unknown')\n",
    "    d[i].classes_ = np.append(d[i].classes_, 'unknown')\n",
    "    \n",
    "fit_test = fit_test.apply(lambda x: d[x.name].transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The input to the keras model come in three parts the numerical columns, the catagorical columns and the time column.\n",
    "The time is treated seperately because the distribution (mean,std) of the variables changes with time. \n",
    "To account for this a time dependent linear offset is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michaelspillane/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "data = Input(shape = (DF_train.shape[1],),name = 'data')\n",
    "tm = Input(shape = (1,),name = 'tm')\n",
    "\n",
    "emb_input = [Input(shape = (1,)) for i in range(fit.shape[1])]  #The catagorical variables to be used in embeddings\n",
    "\n",
    "in_size = (fit.max(axis=0)+2).values.astype(int)  #The number of unique values in the catogorical columns\n",
    "out_size = np.maximum(np.minimum(np.rint(np.log(in_size)),7),2).astype(int)  # the output dimension of the embedding layers\n",
    "\n",
    "emb_list = [Embedding(output_dim=out_size[i], input_dim=in_size[i], input_length=1,embeddings_constraint=unitnorm(axis = 1))(emb_input[i]) for i in range(fit.shape[1])]\n",
    "\n",
    "emb = concatenate(emb_list)\n",
    "\n",
    "emb = Flatten()(emb)\n",
    "            \n",
    "x = concatenate([emb,data,tm])\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "y1 = Dense(out_size.sum()+DF_train.shape[1]+1,activation = 'linear')(tm)\n",
    "\n",
    "x = add([y1,x])  #This adds the linear offset to account for the changing distribution over time\n",
    "x = concatenate([x,tm])\n",
    "x = Dense(200, activation='relu')(x)\n",
    "x = Dropout(.8)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs = [emb_input[i] for i in range(fit.shape[1])]+[data]+[tm],outputs = [x])\n",
    "\n",
    "optimizer = Adam(beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "class_weight = {0: 1.,1: 10.}\n",
    "model.compile(optimizer=optimizer, loss=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michaelspillane/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n"
     ]
    }
   ],
   "source": [
    "model.fit([fit[i] for i in list(fit)]+[X_scaled_train.reshape((X_scaled_train.shape[0],X_scaled_train.shape[1],))]+[X_scaled_time_train],\n",
    "                    y_train,epochs=10,batch_size = 5000,class_weight = class_weight,shuffle=True)\n",
    "Y_pred = model.predict([fit[i] for i in list(fit)]+[X_scaled_train.reshape((X_scaled_train.shape[0],X_scaled_train.shape[1],))]+[X_scaled_time_train])\n",
    "y_pred = model.predict([fit_test[i] for i in list(fit_test)]+[X_scaled_test.reshape((X_scaled_test.shape[0],X_scaled_test.shape[1],))]+[X_scaled_time_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can finally test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2bd3e7f4f450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_train, Y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
